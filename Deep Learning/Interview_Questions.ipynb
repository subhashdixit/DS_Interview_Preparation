{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Interview Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What is the vanishing gradient issue, and how to overcome it? How non-saturating activation helps in reducing it?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Regularization technique used in deep learning. - Drop Out (how it is done during training and testing)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Batch Normalization - how many trainable parameters are there? Why do we do it? How does it help**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. CNN - how it helps in reducing parameters?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Autoencoders and their use**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. TF-IDF**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. CBow and Skim-graph. Why do we do negative sampling?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Bert and Attention Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Neural network structure**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. Fasttext embeddings**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12. Why are initializers used?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**13. What happens if initializers are not used in Neural Networks?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14. What are autoencoders? Explain the different layers of autoencoders and mention three practical usages of them.**\n",
    "\n",
    "**==>>**\n",
    "\n",
    "Autoencoders are a type of deep learning used for unsupervised learning. They have important layers: input, encoder, bottleneck, decoder, and output.\n",
    "\n",
    "The three parts of an autoencoder are as follows:\n",
    "\n",
    "1. Encoder: It makes the input data smaller, compressing it.\n",
    "2. Latent Space Representation/Bottleneck/Code: This is a compact summary of the input, capturing the most important information.\n",
    "3. Decoder: It takes the compressed knowledge and turns it back into the original data. A loss function helps compare input and output images. It's essential that the input and output have the same dimensions, but you can experiment with everything in between.\n",
    "\n",
    "Autoencoders find practical use in various real-world applications, including:\n",
    "\n",
    "- Transformers and Big Bird (where autoencoders are a part): These are used for text summarization and text generation.\n",
    "- Image compression\n",
    "- A nonlinear version of Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**15. What is an activation function and discuss the use of an activation function? Explain three different types of activation functions?**\n",
    "\n",
    "**==>>**\n",
    "\n",
    "In simple terms, the activation function acts like a gate for neurons in a neural network. It decides whether a neuron should be active or not. Without these functions, the network would be too linear and wouldn't capture complex patterns.\n",
    "\n",
    "There are various activation functions:\n",
    "\n",
    "1. Sigmoid: This function gives outputs between 0 and 1, making it suitable for binary classification tasks. However, it has issues like a vanishing gradient and computational expense.\n",
    "\n",
    "2. ReLU (Rectified Linear Unit): ReLU returns the input for positive values and 0 for negatives. It solves the vanishing gradient problem for positive values but not for negatives. It's fast due to its simplicity.\n",
    "\n",
    "3. Leaky ReLU: It handles the vanishing gradient problem by returning a small value for negatives and acts like ReLU for positive values.\n",
    "\n",
    "4. Softmax: Typically used in the final layer for multi-classification, Softmax provides a set of probabilities that add up to 1, making it compatible with the cross-entropy loss commonly used in classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**16. You are using a deep neural network for a prediction task. After training your model, you notice that it is strongly overfitting the training set and that the performance on the test isnâ€™t good. What can you do to reduce overfitting?**\n",
    "\n",
    "**==>>**\n",
    "\n",
    "To prevent overfitting in a deep neural network, we can make changes in three main areas: the input data, the network design, and the training process:\n",
    "\n",
    "1. **Input Data to the Network:**\n",
    "   - Ensure all data features are available and reliable.\n",
    "   - Confirm that the training data distribution matches the validation and test sets. Differences can make predictions challenging.\n",
    "   - Check for any contamination or leakage between the training and validation data.\n",
    "   - Consider using data augmentation techniques to increase the dataset's size if it's too small.\n",
    "   - Ensure that the dataset is balanced.\n",
    "\n",
    "2. **Network Design:**\n",
    "   - Examine the model's complexity. Ask questions about each component:\n",
    "     - Can fully connected layers be replaced with convolutional and pooling layers?\n",
    "     - Why were specific numbers of layers and neurons chosen? Using pre-trained models can simplify this.\n",
    "     - Incorporate regularization techniques like ridge (l1), lasso (l2), or elastic net (both).\n",
    "     - Introduce dropouts and batch normalization.\n",
    "\n",
    "3. **Training Process:**\n",
    "   - Decide when to stop training based on improvements in validation losses. Utilize callbacks for early stopping when the validation loss stabilizes, and consider using the \"restore_best_weights\" option.\n",
    "\n",
    "Simplifying these steps can help avoid overfitting in deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://yousefhosni.medium.com/deep-learning-interview-questions-answers-439163d3fc02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
